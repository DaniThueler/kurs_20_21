{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "    - Pattern\n",
    "    - TextBlob\n",
    "- POS Tagging\n",
    "    - Pattern\n",
    "    - TextBlob\n",
    "- Stop Word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "- Wie splittet man einfach einen Satz in Wörter auf?\n",
    "- https://www.admin.ch/opc/de/classified-compilation/19995395/index.html#a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verfassung = '''\n",
    "Im Namen Gottes des Allmächtigen!\n",
    "\n",
    "Das Schweizervolk und die Kantone,\n",
    "\n",
    "in der Verantwortung gegenüber der Schöpfung,\n",
    "\n",
    "im Bestreben, den Bund zu erneuern, um Freiheit und Demokratie, Unabhängigkeit und Frieden in Solidarität und Offenheit gegenüber der Welt zu stärken,\n",
    "\n",
    "im Willen, in gegenseitiger Rücksichtnahme und Achtung ihre Vielfalt in der Einheit zu leben,\n",
    "\n",
    "im Bewusstsein der gemeinsamen Errungenschaften und der Verantwortung gegenüber den künftigen Generationen,\n",
    "\n",
    "gewiss, dass frei nur ist, wer seine Freiheit gebraucht, und dass die Stärke des Volkes sich misst am Wohl der Schwachen,\n",
    "\n",
    "geben sich folgende Verfassung:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verfassung = verfassung.decode('utf-8')\n",
    "verfassung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In einzelne Wörter + Zeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(verfassung)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(verfassung)\n",
    "words=[word.lower() for word in words if word.isalpha()]\n",
    "words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hier noch ein paar praktische Python funktionen wie isalpha\n",
    "\n",
    "|  s.startswith(t) | test if s starts with t |\n",
    "|  ------ | ------ |\n",
    "|  s.endswith(t) | test if s ends with t |\n",
    "|  t in s | test if t is a substring of s |\n",
    "|  s.islower() | test if s contains cased characters and all are lowercase |\n",
    "|  s.isupper() | test if s contains cased characters and all are uppercase |\n",
    "|  s.isalpha() | test if s is non-empty and all characters in s are alphabetic |\n",
    "|  s.isalnum() | test if s is non-empty and all characters in s are alphanumeric |\n",
    "|  s.isdigit() | test if s is non-empty and all characters in s are digits |\n",
    "|  s.istitle() | test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Sätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(verfassung)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- d.h. das überführen von Wörtern in den gleichen Stamm. \n",
    "- Übliche Probleme die Sprache so mit sich bringt:\n",
    "    - Deklination: gehe, gehst, gehen, geht, ... es handelt sich um das gleiche Wort. \n",
    "    - Plural, Groß/Kleinschreibung: katze, katzen, Katze ... es handelt sich auch um das gleiche Wort. \n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter stemmer\n",
    "- geht ganz ok für Englisch\n",
    "- https://de.wikipedia.org/wiki/Porter-Stemmer-Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "engl_words = udhr.words(\"English-Latin1\")\n",
    "engl_words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[porter.stem(w) for w in engl_words][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer\n",
    "- kommt zu leicht anderen \"stämmen\"\n",
    "- funktioniert nicht so gut für deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(w) for w in engl_words][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball stemmer\n",
    "- Geht gut für deusch\n",
    "- http://snowball.tartarus.org/algorithms/german/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = nltk.stem.snowball.GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[snowball.stem(w) for w in words][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer\n",
    "- Zurückführen des Textes zu \"Wörterbuchdefinitionen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Women in technologies areas are amazing at coding. Especially one woman is great, her name is Ursula Burns.\"\n",
    "words = word_tokenize(text)\n",
    "words_engl =[word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer() \n",
    "lemmas = [lemma.lemmatize(i) for i in words_engl]\n",
    "lemmas[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizer Deutsch NLTK\n",
    "- Geht leider nicht für deusch :( in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_text = '''\n",
    "Kurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. \n",
    "'''\n",
    "#party_text = party_text.decode(\"UTF-8\")\n",
    "words = word_tokenize(party_text)\n",
    "words_german =[word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer() \n",
    "lemmas = [lemma.lemmatize(i) for i in words_german]\n",
    "lemmas[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer in Textblob\n",
    " - https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    " - ```pip install -U textblob```\n",
    " - ```pip install -U textblob-de```\n",
    " - https://github.com/markuskiller/textblob-de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_text = '''\n",
    "Kurz vor 22.00 Uhr sind Polizisten der Stadtpolizei in alle Richtungen aber vor allem in Richtung Kreis 9 ausgerückt. Grund war eine Meldung zu einer privaten Party an der Aargauerstrasse, die angeblich ausser Kontrolle geraten sei. \n",
    "'''\n",
    "#party_text = party_text.decode(\"UTF-8\")\n",
    "from textblob_de import TextBlobDE\n",
    "from textblob_de import PatternParser\n",
    "blob = TextBlobDE(party_text, parser=PatternParser(pprint=False, lemmata=True))\n",
    "blob.parse()\n",
    "list(blob.words.lemmatize())[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy German Lemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spacy ist ein machine learning natural language processing tool. \n",
    "- Aufgrund des Deep Learning Ansatzes ist es am weitesten von allen derzeitigen Tools fortgeschritten. \n",
    "- Allerdings erschliessen sich viele Funktionen für Laien nicht wirklich. \n",
    "\n",
    "- ```pip install --user spacy```\n",
    "- Deutsche Modelle runterladen:```pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz```\n",
    "- oder ```python -m spacy download de_core_news_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install german-lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp('Die Schweizer Wirtschaft dürfte 2020 ein Wachstum von zwei Prozent verzeichnen, sagen die Datenschutzbeauftragten. '\n",
    "          'Dasselbe gilt für Versicherungsprämien und Aufwände der Krankenkassen im Allgemeinen. Auch die Bäume haben ein Problem.')\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging (Part of Speech Tagging)\n",
    "- Ich bin nur interessiert an Substantiven oder Adjektiven oder Verben. \n",
    "- Wie kann ich nur diese filtern? \n",
    "- Funktioniert für Deutsch nicht auf Anhieb in NLTK :(\n",
    "- aber gut in spacy und textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übersicht\n",
    "https://www.clips.uantwerpen.be/pages/mbsp-tags\n",
    "\n",
    "|  Tag | Description | Example |\n",
    "|  ------ | ------ | ------ |\n",
    "|  **CC** | conjunction, coordinating | and, or, but |\n",
    "|  **CD** | cardinal number | five, three, 13% |\n",
    "|  **DT** | determiner | the, a, these |\n",
    "|  **EX** | existential there | there were six boys |\n",
    "|  **FW** | foreign word | mais |\n",
    "|  **IN** | conjunction, subordinating or preposition | of, on, before, unless |\n",
    "|  **JJ** | adjective | nice, easy |\n",
    "|  **JJR** | adjective, comparative | nicer, easier |\n",
    "|  **JJS** | adjective, superlative | nicest, easiest |\n",
    "|  **LS** | list item marker |  |\n",
    "|  **MD** | verb, modal auxillary | may, should |\n",
    "|  **NN** | noun, singular or mass | tiger, chair, laughter |\n",
    "|  **NNS** | noun, plural | tigers, chairs, insects |\n",
    "|  **NNP** | noun, proper singular | Germany, God, Alice |\n",
    "|  **NNPS** | noun, proper plural | we met two Christmases ago |\n",
    "|  **PDT** | predeterminer | both his children |\n",
    "|  **POS** | possessive ending | s |\n",
    "|  **PRP** | pronoun, personal | me, you, it |\n",
    "|  **PRP** | pronoun, possessive | my, your, our |\n",
    "|  **RB** | adverb | extremely, loudly, hard |\n",
    "|  **RBR** | adverb, comparative | better |\n",
    "|  **RBS** | adverb, superlative | best |\n",
    "|  **RP** | adverb, particle | about, off, up |\n",
    "|  **SYM** | symbol | % |\n",
    "|  **TO** | infinitival to | what to do? |\n",
    "|  **UH** | interjection | oh, oops, gosh |\n",
    "|  **VB** | verb, base form | think |\n",
    "|  **VBZ** | verb, 3rd person singular present | she thinks |\n",
    "|  **VBP** | verb, non-3rd person singular present | I think |\n",
    "|  **VBD** | verb, past tense | they thought |\n",
    "|  **VBN** | verb, past participle | a sunken ship |\n",
    "|  **VBG** | verb, gerund or present participle | thinking is fun |\n",
    "|  **WDT** | wh-determiner | which, whatever, whichever |\n",
    "|  **WP** | wh-pronoun, personal | what, who, whom |\n",
    "|  **WP** | wh-pronoun, possessive | whose, whosever |\n",
    "|  **WRB** | wh-adverb | where, when |\n",
    "|  **.** | punctuation mark, sentence closer | .;?* |\n",
    "|  **,** | punctuation mark, comma | , |\n",
    "|  **:** | punctuation mark, colon | : |\n",
    "|  **(** | contextual separator, left paren | ( |\n",
    "|  **)** | contextual separator, right paren | ) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tags in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verben = []\n",
    "substantive = []\n",
    "adjektive = []\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp(party_text)\n",
    "for token in doc:\n",
    "    if \"VERB\" in token.pos_:\n",
    "        verben.append(token)\n",
    "    if \"NOUN\" in token.pos_:\n",
    "        substantive.append(token)\n",
    "    if \"ADJ\" in token.pos_:\n",
    "        adjektive.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substantive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjektive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags in textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substantive = []\n",
    "adjektive = []\n",
    "verben = []\n",
    "blob = TextBlobDE(party_text, parser=PatternParser(pprint=False, lemmata=True))\n",
    "for word in blob.tags:\n",
    "    if \"NN\" in word[1]:\n",
    "        substantive.append(word[0])\n",
    "    if \"V\" in word[1]:\n",
    "        verben.append(word[0])\n",
    "    if \"JJ\" in word[1]:\n",
    "        adjektive.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substantive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjektive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Englisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_engl = [\"the\", \"big\", \"tree\", \"and\", \"the\", \"duck\"]\n",
    "filtered_words = [word for word in words_engl if word not in stopwords.words('english')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_list = list(set(mylist)- set([\"ich\"]))\n",
    "my_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_german = [\"ich\", \"war\", \"im\", \"wald\", \"spazieren\"]\n",
    "filtered_words = [word for word in words_german if word not in stopwords.words('german')]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordle 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\" \").join(udhr.words(\"German_Deutsch-Latin1\"))\n",
    "# tokenize and lemmatize\n",
    "words = word_tokenize(text)\n",
    "words_german =[word.lower() for word in words if word.isalpha()]\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "lemmas = [lemma.lemmatize(i) for i in words_german]\n",
    "clean_blob = lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numbers etc.. removal\n",
    "- Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words_alpha =[word.lower() for word in clean_blob if word.isalpha()]\n",
    "stopwords = stopwords.words('german')\n",
    "stopwords.append(\"jede\")\n",
    "result = [word for word in words_alpha if word not in stopwords]\n",
    "result[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\" \").join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = [123,\n",
    "     123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import path\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "\n",
    "maske = np.array(Image.open(path.join(d, \"maske.png\")))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=maske, contour_width=3, contour_color='steelblue',collocations=False, normalize_plurals=False)\n",
    "wc.generate(text)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
